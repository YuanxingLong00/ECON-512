
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Class 4: Quasi-Newton Methods</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-10-09"><meta name="DC.source" content="quasiNewt.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Class 4: Quasi-Newton Methods</h1><!--introduction--><div><ul><li>Last week we introduced some methods for solving nonlinear equations, the workhorse method was Newton's method, which relied on iteratively linearlizing the nonlinear problem around an iterate. Leading to the iteration rule:</li></ul></div><p><img src="quasiNewt_eq09773037663860916575.png" alt="$$ x^{(k+1)} \leftarrow x^{(k)} - [f'(x^{(k)})]^{-1}f(x^{(k)}) $$" style="width:157px;height:14px;"></p><div><ul><li>One issue with Newton's method is that it required computing the Jacobian (matrix of derivatives) of the problem at every iteration, <img src="quasiNewt_eq06332757189948181325.png" alt="$[f'(x^{(k)})]^{-1}$" style="width:49px;height:13px;">. While this can be done numerically it may be computatitionally intensive.</li></ul></div><div><ul><li>Quasi-Newton methods are simply approaches to approximate the jacobian rather than computing it directly (either numerically or analytically).</li></ul></div><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Secant Method</a></li><li><a href="#4">Broyden's Method</a></li><li><a href="#7">What's going wrong?</a></li><li><a href="#8">Gravity with Gravitas</a></li></ul></div><h2 id="1">Secant Method</h2><p>Suppose <img src="quasiNewt_eq08238684485259328926.png" alt="$f : R \rightarrow R$" style="width:50px;height:10px;"> is univariate.</p><p>While computing the numerical derivative would only require a single function evaluation, we can save even that by just using the previous iterate:</p><p><img src="quasiNewt_eq16267582881153919837.png" alt="$$f'(x^{(k)}) \approx \frac{ f(x^{(k)}) - f(x^{(k-1)}) }{ x^{(k)} -&#xA;x^{(k-1)}}$$" style="width:139px;height:26px;"></p><p>The secant method iteration just replaces the derivative in Newtons method with this secant approximation:</p><p><img src="quasiNewt_eq08468748400564528474.png" alt="$$x^{(k+1)} \leftarrow x^{(k)} - \frac{ x^{(k)} - x^{(k-1)} }{ f(x^{(k)})&#xA;- f(x^{(k-1)}) } f(x^{(k)})$$" style="width:196px;height:28px;"></p><p>This illustrates the main concept of quasi-newton methods: use previously computed information to efficiently approximate the derivative information of the current iterate.</p><p>Formally, the secant method requires 2 initial guesses. Although often we just compute the derivative for the first iteration.</p><p>Let's recall our univariate function:</p><pre class="codeinput">f = @(x) 2 + exp(x) - 3.*(x.^2);
X = -2:.1:4;
<span class="comment">%Fx = 2 + exp(X) - 3.*(X.^2);</span>
plot(X, f(X), X, zeros(size(X)))
</pre><img vspace="5" hspace="5" src="quasiNewt_01.png" alt=""> <p>So to implement the secant method:</p><pre class="codeinput"><span class="comment">% Assign initial values</span>
x = 0;
xOld = 1;
fOld = f(xOld);

<span class="comment">% Secant iterations:</span>
tol = 1e-8;
maxit = 100;
<span class="keyword">for</span> iter =1:maxit
    fVal = f(x);
    fprintf(<span class="string">'iter %d: x = %.8f, f(x) = %.8f\n'</span>, iter, x, fVal);
    <span class="keyword">if</span> abs(fVal) &lt; tol
        <span class="keyword">break</span>
    <span class="keyword">else</span>
        xNew = x - ( (x - xOld) / (fVal - fOld) )* fVal;
        xOld = x;
        x = xNew;
        fOld = fVal;
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">iter 1: x = 0.00000000, f(x) = 3.00000000
iter 2: x = 2.34060815, f(x) = -4.04778771
iter 3: x = 0.99631611, f(x) = 1.73034902
iter 4: x = 1.39888437, f(x) = 0.18004596
iter 5: x = 1.44563702, f(x) = -0.02504405
iter 6: x = 1.43992794, f(x) = 0.00021426
iter 7: x = 1.43997637, f(x) = 0.00000024
iter 8: x = 1.43997643, f(x) = -0.00000000
</pre><h2 id="4">Broyden's Method</h2><p>Broyden's method generalizes the secant method to a multidimensional problem. However, how do we update the entire Jacobian with a single pair of function evaluations.</p><p>Effectively, for two values of <img src="quasiNewt_eq17698763224993081805.png" alt="$f:R^n \rightarrow R^n$" style="width:59px;height:10px;"> we need to solve</p><p><img src="quasiNewt_eq03603359961140205402.png" alt="$$ f(x^{(k)}) - f(x^{(k-1)}) = A (x^{(k)} - x^{(k)}) $$" style="width:166px;height:14px;"></p><p>But <img src="quasiNewt_eq05147331747641807187.png" alt="$A$" style="width:8px;height:8px;"> has <img src="quasiNewt_eq04535246925673468418.png" alt="$n^2$" style="width:10px;height:10px;"> values, and we only have <img src="quasiNewt_eq08984225997457563733.png" alt="$n$" style="width:7px;height:6px;"> equations.</p><p>We will supplement this with information from the previous approxmiation of the Jacobian, which we hope is "close" to the current Jacobian. Where close is determined according to the <a href="https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm">Frobenius norm</a>.</p><p>This leads to the iteration rule:</p><p><img src="quasiNewt_eq11713093371336680229.png" alt="$$A^{(k+1)} \leftarrow A^{(k)} + [ f(x^{(k+1)}) - f(x^{(k)}) - A^{(k)}d^{(k)}] $$&#xA;$$\frac{ d^{(k)^T} }{ d^{(k)^T} d^{(k)} } $$" style="width:264px;height:26px;"></p><p>Where <img src="quasiNewt_eq02750737470359646799.png" alt="$d^{(k)}$" style="width:15px;height:11px;"> is the column vector <img src="quasiNewt_eq15308931047397192103.png" alt="$x^{(k+1)} - x^{(k)}$" style="width:56px;height:11px;">. Note that the term in brackets is <img src="quasiNewt_eq17754908661825192061.png" alt="$n \times 1$" style="width:25px;height:8px;"> while the fraction is <img src="quasiNewt_eq01711202362185807343.png" alt="$1 \times n$" style="width:25px;height:8px;">.</p><p>We could use <img src="quasiNewt_eq15662396307546603914.png" alt="$A^{(k+1)}$" style="width:28px;height:10px;"> directly, but then we would still need to solve a linear equation to get the next iterate. Instead, it turns out a matrix multiplication can directly deliver us an approximation of the inverse Jacobian. Then we have:</p><p><img src="quasiNewt_eq13816639442816740809.png" alt="$$ B^{(k+1)} \leftarrow B^{(k)} + [ (d^{(k)} - u^{(k)}) d^{(k)^T}B^{(k)}]/(d^{(k)^T} u^{(k)})$$" style="width:234px;height:14px;"></p><p>Working this out will take some algebra, but at least note that the denominator here is a scalar, so it is just a matrix multiplication in practice.</p><p>Let's implement it to solve our cournot model from last time. We'll need an initial guess at the Jacobian, so we'll use a numerical derivative:</p><pre class="codeinput">q = [2; 3];
fVal = cournot(q)
iJac = inv(myJac(<span class="string">'cournot'</span>, q))
</pre><pre class="codeoutput">
fVal =

   -0.9257
   -2.1714


iJac =

   -1.5113    0.0286
    0.0020   -1.1809

</pre><p>Now for the Broyden iterations:</p><pre class="codeinput">maxit = 100;
tol = 1e-6;
<span class="keyword">for</span> iter = 1:maxit
    fnorm = norm(fVal);
    fprintf(<span class="string">'iter %d: q(1) = %f, q(2) = %f, norm(f(x)) = %.8f\n'</span>, iter, q(1), q(2), norm(fVal));
    <span class="keyword">if</span> norm(fVal) &lt; tol
        <span class="keyword">break</span>
    <span class="keyword">end</span>
    d = - (iJac * fVal);
    q = q+d;
    fOld = fVal;
    fVal = cournot(q);
    u = iJac*(fVal - fOld);
    iJac = iJac + ( (d - u) * (d'*iJac) )/ (d'*u);
<span class="keyword">end</span>
</pre><pre class="codeoutput">iter 1: q(1) = 2.000000, q(2) = 3.000000, norm(f(x)) = 2.36051746
iter 2: q(1) = 0.662994, q(2) = 0.437703, norm(f(x)) = 0.40465064
iter 3: q(1) = 0.898254, q(2) = 0.797154, norm(f(x)) = 0.14241697
iter 4: q(1) = 0.846374, q(2) = 0.699278, norm(f(x)) = 0.01472917
iter 5: q(1) = 0.839108, q(2) = 0.688562, norm(f(x)) = 0.00054648
iter 6: q(1) = 0.839652, q(2) = 0.688757, norm(f(x)) = 0.00008842
iter 7: q(1) = 0.839552, q(2) = 0.688804, norm(f(x)) = 0.00001637
iter 8: q(1) = 0.839568, q(2) = 0.688796, norm(f(x)) = 0.00000006
</pre><p>You will recall that using the derivative information directly took fewer iterations to converge, however each iteration was more computationally intensive since</p><div><ol><li>Had to compute numerical Jacobian (or supply analytic Jacobian).</li><li>Had to solve a linear equation as part of iteration.</li></ol></div><h2 id="7">What's going wrong?</h2><p>Inevitably, when you are trying to solve a nonlinear system, its not going to work (at least the first few attempts). Just remember... <b>it's all your fault</b>.</p><div><ol><li>Using a packaged solver can minimize coding errors in the solution algorithm itself.</li><li>If you are coding your Jacobian, its a good idea to at least check your code against a numerical derivative.</li><li>Check the coding of your function by computing it at some points where you can calculate the answer with paper and pencil.</li><li>"Explore" (plot or grid search) your algorithm to attempt to find a good start point.</li><li>Re-scale your function to avoid ill-conditioning. Try to keep "reasonable inputs" in the same order of magnitude.</li><li>Be mindful of bounds, if you have a <img src="quasiNewt_eq08953528819398113250.png" alt="$log(x)$" style="width:29px;height:11px;"> in your equations, you defintely don't want to evaluate at <img src="quasiNewt_eq05658435132098161995.png" alt="$x = -2$" style="width:35px;height:8px;">, but the solver won't realize that unless you tell it.</li><li>If you try to solve a system with no solution, the computer is not going to tell you this, it will just keep trying.</li><li>If you try to solve a system with kinks or discontinuities, your mileage may vary (to put it mildly).</li><li>Finally, Newton's method can always blame you for not being in the neighborhood of the solution.</li></ol></div><p>Some last advice, sometimes you can transform your equations to make them closer to linear. In the extreme why solve:</p><p><img src="quasiNewt_eq12885405268351295356.png" alt="$$ \exp(x) - 10 = 0 $$" style="width:76px;height:11px;"></p><p>When you can solve:</p><p><img src="quasiNewt_eq09861172492230546386.png" alt="$$ x = log(10)$$" style="width:54px;height:11px;"></p><p>However it may also be that a system like:</p><p><img src="quasiNewt_eq00639419250639511840.png" alt="$$ x^{0.2} + y^{0.2} - 2 = 0 $$" style="width:85px;height:12px;"></p><pre class="codeinput"><span class="comment">% $$ x^{0.1} + y^{0.4} - 2 = 0 $$</span>
<span class="comment">%</span>
<span class="comment">% Can be more easily be solved after re-scaling to get closer to CRTS:</span>
<span class="comment">%</span>
<span class="comment">% $$ (x^{0.2} + y^{0.2})^5 - 32 = 0 $$</span>
<span class="comment">%</span>
<span class="comment">% $$ (x^{0.1} + y^{0.4})^4 - 16 = 0 $$</span>
<span class="comment">%</span>
<span class="comment">% The broad lesson here is that the computer wants to solve the math</span>
<span class="comment">% problem you give it, if that problem needs to be manipulated slightly in</span>
<span class="comment">% ways that are not "economically intuitive" that is fine. In general, the</span>
<span class="comment">% closer to a _linear_ problem you have, the easier it will be to solve</span>
<span class="comment">% with Newton or quasi-Newton methods, since both follow the principle of</span>
<span class="comment">% successive linearization.</span>
<span class="comment">%</span>
</pre><h2 id="8">Gravity with Gravitas</h2><p>This class is short, but next class is our first applicaiton, so I'll use some of the time to set it up.</p><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Class 4: Quasi-Newton Methods 
%
% * Last week we introduced some methods for solving nonlinear equations, the
% workhorse method was Newton's method, which relied on iteratively
% linearlizing the nonlinear problem around an iterate. Leading to the
% iteration rule:
%
% $$ x^{(k+1)} \leftarrow x^{(k)} - [f'(x^{(k)})]^{-1}f(x^{(k)}) $$
%
% * One issue with Newton's method is that it required computing the Jacobian
% (matrix of derivatives) of the problem at every iteration, $[f'(x^{(k)})]^{-1}$. 
% While this can
% be done numerically it may be computatitionally intensive. 
%
% * Quasi-Newton methods are simply approaches to approximate the jacobian
% rather than computing it directly (either numerically or analytically). 

%% Secant Method
%
% Suppose $f : R \rightarrow R$ is univariate. 
%
% While computing the numerical derivative would only require a single
% function evaluation, we can save even that by just using the previous
% iterate: 
%
% $$f'(x^{(k)}) \approx \frac{ f(x^{(k)}) - f(x^{(k-1)}) }{ x^{(k)} -
% x^{(k-1)}}$$
%
% The secant method iteration just replaces the derivative in Newtons
% method with this secant approximation: 
%
% $$x^{(k+1)} \leftarrow x^{(k)} - \frac{ x^{(k)} - x^{(k-1)} }{ f(x^{(k)})
% - f(x^{(k-1)}) } f(x^{(k)})$$
%
% This illustrates the main concept of quasi-newton methods: use previously
% computed information to efficiently approximate the derivative
% information of the current iterate. 
%
% Formally, the secant method requires 2 initial guesses. Although often
% we just compute the derivative for the first iteration.   

%% 
% Let's recall our univariate function:
f = @(x) 2 + exp(x) - 3.*(x.^2);
X = -2:.1:4;
%Fx = 2 + exp(X) - 3.*(X.^2);
plot(X, f(X), X, zeros(size(X)))

%% 
% So to implement the secant method:

% Assign initial values
x = 0;
xOld = 1;
fOld = f(xOld);

% Secant iterations:
tol = 1e-8;
maxit = 100;
for iter =1:maxit
    fVal = f(x);
    fprintf('iter %d: x = %.8f, f(x) = %.8f\n', iter, x, fVal);
    if abs(fVal) < tol
        break
    else
        xNew = x - ( (x - xOld) / (fVal - fOld) )* fVal;
        xOld = x;
        x = xNew;
        fOld = fVal;       
    end
end


%% Broyden's Method
%
% Broyden's method generalizes the secant method to a multidimensional
% problem. However, how do we update the entire Jacobian with a single pair
% of function evaluations. 
%
% Effectively, for two values of $f:R^n \rightarrow R^n$ we need to solve 
%
% $$ f(x^{(k)}) - f(x^{(k-1)}) = A (x^{(k)} - x^{(k)}) $$
%
% But $A$ has $n^2$ values, and we only have $n$ equations. 
%
% We will supplement this with information from the previous approxmiation
% of the Jacobian, which we hope is "close" to the current Jacobian.
% Where close is determined according to the
% <https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm Frobenius norm>.
%
% This leads to the iteration rule: 
%
% $$A^{(k+1)} \leftarrow A^{(k)} + [ f(x^{(k+1)}) - f(x^{(k)}) - A^{(k)}d^{(k)}] $$
% $$\frac{ d^{(k)^T} }{ d^{(k)^T} d^{(k)} } $$
%
% Where $d^{(k)}$ is the column vector $x^{(k+1)} - x^{(k)}$. Note that the
% term in brackets is $n \times 1$ while the fraction is $1 \times n$. 
%
% We could use $A^{(k+1)}$ directly, but then we would still need to solve
% a linear equation to get the next iterate. Instead, it turns out a matrix
% multiplication can directly deliver us an approximation of the inverse
% Jacobian. Then we have: 
%
% $$ B^{(k+1)} \leftarrow B^{(k)} + [ (d^{(k)} - u^{(k)}) d^{(k)^T}B^{(k)}]/(d^{(k)^T} u^{(k)})$$
%
% Working this out will take some algebra, but at least note that the
% denominator here is a scalar, so it is just a matrix multiplication in
% practice. 
%
% Let's implement it to solve our cournot model from last time. We'll need
% an initial guess at the Jacobian, so we'll use a numerical derivative:
q = [2; 3];
fVal = cournot(q)
iJac = inv(myJac('cournot', q))
%%
%
% Now for the Broyden iterations: 
maxit = 100; 
tol = 1e-6; 
for iter = 1:maxit
    fnorm = norm(fVal);
    fprintf('iter %d: q(1) = %f, q(2) = %f, norm(f(x)) = %.8f\n', iter, q(1), q(2), norm(fVal));
    if norm(fVal) < tol
        break
    end
    d = - (iJac * fVal);
    q = q+d;
    fOld = fVal;
    fVal = cournot(q);
    u = iJac*(fVal - fOld);
    iJac = iJac + ( (d - u) * (d'*iJac) )/ (d'*u);
end

%% 
% You will recall that using the derivative information directly took fewer
% iterations to converge, however each iteration was more computationally
% intensive since 
% 
% # Had to compute numerical Jacobian (or supply analytic Jacobian).
% # Had to solve a linear equation as part of iteration. 

%% What's going wrong? 
%
% Inevitably, when you are trying to solve a nonlinear system, its not
% going to work (at least the first few attempts). Just remember... 
% *it's all your fault*.
%
% # Using a packaged solver can minimize coding errors in the solution
% algorithm itself. 
% # If you are coding your Jacobian, its a good idea to at least check your
% code against a numerical derivative. 
% # Check the coding of your function by computing it at some points where
% you can calculate the answer with paper and pencil. 
% # "Explore" (plot or grid search) your algorithm to attempt to find a
% good start point. 
% # Re-scale your function to avoid ill-conditioning. Try to keep
% "reasonable inputs" in the same order of magnitude. 
% # Be mindful of bounds, if you have a $log(x)$ in your equations, you
% defintely don't want to evaluate at $x = -2$, but the solver won't
% realize that unless you tell it. 
% # If you try to solve a system with no solution, the computer is not
% going to tell you this, it will just keep trying.
% # If you try to solve a system with kinks or discontinuities, your mileage
% may vary (to put it mildly).
% # Finally, Newton's method can always blame you for not being in the
% neighborhood of the solution. 
%
% Some last advice, sometimes you can transform your equations to make them closer to
% linear. In the extreme why solve: 
%
% $$ \exp(x) - 10 = 0 $$   
%
% When you can solve: 
%
% $$ x = log(10)$$
%
% However it may also be that a system like:
% 
% $$ x^{0.2} + y^{0.2} - 2 = 0 $$

% $$ x^{0.1} + y^{0.4} - 2 = 0 $$
%
% Can be more easily be solved after re-scaling to get closer to CRTS:
% 
% $$ (x^{0.2} + y^{0.2})^5 - 32 = 0 $$
%
% $$ (x^{0.1} + y^{0.4})^4 - 16 = 0 $$
%
% The broad lesson here is that the computer wants to solve the math
% problem you give it, if that problem needs to be manipulated slightly in
% ways that are not "economically intuitive" that is fine. In general, the
% closer to a _linear_ problem you have, the easier it will be to solve
% with Newton or quasi-Newton methods, since both follow the principle of
% successive linearization. 
%

%% Gravity with Gravitas
%
% This class is short, but next class is our first applicaiton, so I'll use
% some of the time to set it up. 
##### SOURCE END #####
--></body></html>